- 这个是fp16精度的矩阵乘法
- 我们需要思考的问题是：到底fp16产生的中间变量应该是fp32还是fp16！
    - 我觉得是中间变量应该是fp32，但是最后结果还得是fp16吧！
- cublasHgemm仅接收参数都是fp16的指针，但是累加的中间变量到底是fp32，还是fp16呢，这个函数没有说明是哪一个，我觉得大概率是fp32！



# # 512 * 512 * 512 的测试，测试下 fp16 的性能吧！

| T4机器             | time/ms  | max diff |
| ------------------ | -------- | -------- |
| cublas             | 0.331936 | 0.013945 |
| cutlass/tensorcore | 0.473632 | 0.003209 |
| cutlass            | 2.704832 | 0.066204 |
| matmul_gpu         | 7.870624 | 0.003209 |
| wmma               | 5.833024 | 0.023292 |


| GeForce RTX 2080  SUPER | time/ms  | max diff |
| ----------------------- | -------- | -------- |
| cublas                  | 0.335424 | 0.035730 |
| cutlass                 | 0.854656 | 0.066204 |
| matmul_gpu              | 2.934752 | 0.023292 |
| wmma                    | 0.682976 | 0.023292 |

- 为什么fp16比cublas这么差啊啊啊啊啊啊！蛋疼
  - 但是显然T4 的差距更大



# 1024 * 1024 * 512 的测试

- 这个情况下，cublas肯定不是用的split-K算法
- 让我们看看这个情况下

| T4机器             | time/ms  | max diff |
| ------------------ | -------- | -------- |
| cublas             | 0.811136 | 0.026291 |
| cutlass/tensorcore | 0.862720 | 0.003340 |
